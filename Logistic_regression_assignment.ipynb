{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQPunhQh4RqCkjVGhnRWAy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prachimethi/assignment/blob/main/Logistic_regression_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**THEORITICAL QUESTIONS**"
      ],
      "metadata": {
        "id": "xMum6pMCJ8GA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-1 What is Logistic Regression, and how does it differ from Linear Regression?"
      ],
      "metadata": {
        "id": "nRlMK6Z0KJ4y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression and Linear Regression are both statistical techniques used in machine learning and data analysis, but they serve different purposes and are based on different mathematical models.\n",
        "\n",
        "Linear Regression:\n",
        "\n",
        " It is used to predict a continuous numerical outcome (dependent variable) based on one or more independent variables. The model assumes a linear relationship between the dependent variable\n",
        "ùë¶\n",
        "y and the independent variable(s)\n",
        "ùë•\n",
        "x. It tries to fit a straight line to the data.\n",
        "\n",
        "Logistic Regression:\n",
        "\n",
        "It is used for classification tasks, especially when the dependent variable is categorical (often binary, i.e., 0 or 1).Logistic regression predicts probabilities that an instance belongs to a particular class (e.g., 0 or 1), rather than predicting a continuous value."
      ],
      "metadata": {
        "id": "ZCcAlpAmKPeO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-2  What is the mathematical equation of Logistic Regression ?"
      ],
      "metadata": {
        "id": "OckqRxw_K1Rz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The equation is similar to linear regression but passed through a logistic (sigmoid) function to map the result between 0 and 1:\n",
        "\n",
        "ùëù\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "ùëí\n",
        "‚àí\n",
        "(\n",
        "ùõΩ\n",
        "0\n",
        "+\n",
        "ùõΩ\n",
        "1\n",
        "ùë•\n",
        "1\n",
        "+\n",
        "ùõΩ\n",
        "2\n",
        "ùë•\n",
        "2\n",
        "+\n",
        "‚ãØ\n",
        "+\n",
        "ùõΩ\n",
        "ùëõ\n",
        "ùë•\n",
        "ùëõ\n",
        ")\n",
        "p=\n",
        "1+e\n",
        "‚àí(Œ≤\n",
        "0\n",
        "‚Äã\n",
        " +Œ≤\n",
        "1\n",
        "‚Äã\n",
        " x\n",
        "1\n",
        "‚Äã\n",
        " +Œ≤\n",
        "2\n",
        "‚Äã\n",
        " x\n",
        "2\n",
        "‚Äã\n",
        " +‚ãØ+Œ≤\n",
        "n\n",
        "‚Äã\n",
        " x\n",
        "n\n",
        "‚Äã\n",
        " )\n",
        "\n",
        "1\n",
        "‚Äã\n",
        "\n",
        "Where:\n",
        "\n",
        "ùëù\n",
        "p is the predicted probability of the class being 1,\n",
        "\n",
        "\n",
        "ùõΩ\n",
        "0\n",
        ",\n",
        "ùõΩ\n",
        "1\n",
        ",\n",
        "‚Ä¶\n",
        "Œ≤\n",
        "0\n",
        "‚Äã\n",
        " ,Œ≤\n",
        "1\n",
        "‚Äã\n",
        " ,‚Ä¶ are the model parameters,\n",
        "\n",
        "\n",
        "ùë•\n",
        "1\n",
        ",\n",
        "ùë•\n",
        "2\n",
        ",\n",
        "‚Ä¶\n",
        "x\n",
        "1\n",
        "‚Äã\n",
        " ,x\n",
        "2\n",
        "‚Äã\n",
        " ,‚Ä¶ are the independent variables."
      ],
      "metadata": {
        "id": "N8_1VPcQK6_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-3 Why do we use the Sigmoid function in Logistic Regression ?"
      ],
      "metadata": {
        "id": "vmcn7BpiLNxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the sigmoid function in logistic regression because it allows us to model the probability that an observation belongs to a particular class, which is crucial for classification tasks. The sigmoid function transforms the output of the linear regression equation into a value between 0 and 1, making it interpretable as a probability."
      ],
      "metadata": {
        "id": "TEbx6-8RL0Wz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-4 What is the cost function of Logistic Regression?"
      ],
      "metadata": {
        "id": "LWEMugb0MDKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In logistic regression, the cost function (also called the loss function) is used to measure how well the model's predictions match the actual outcomes (or labels) in the training dataset. The goal of logistic regression is to minimize this cost function, thus making the model's predictions as accurate as possible.\n",
        "\n",
        "For logistic regression, the cost function is based on the log-likelihood or cross-entropy between the predicted probabilities and the true class labels. The most common cost function used for logistic regression."
      ],
      "metadata": {
        "id": "5hr3nZUuMsy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-5 What is Regularization in Logistic Regression? Why is it needed?"
      ],
      "metadata": {
        "id": "xAbn8_6gM4LY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization in logistic regression is a technique used to prevent the model from overfitting, especially when the model has many features or when the data is noisy. Regularization modifies the cost function to penalize large coefficients (weights) in the model, discouraging the model from becoming overly complex or sensitive to the training data.\n",
        "\n",
        "Regularization is Needed:\n",
        "1. Overfitting Prevention\n",
        "2. Feature selection\n",
        "3. Handling multicollinearity\n"
      ],
      "metadata": {
        "id": "r44OQ8QeM9Cj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-six  Explain the difference between Lasso, Ridge, and Elastic Net regression?"
      ],
      "metadata": {
        "id": "2e6_RLGENhiT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In linear regression, Lasso, Ridge, and Elastic Net are all regularization techniques that add penalties to the cost function to prevent overfitting and encourage simpler models. Each of these methods applies different regularization strategies, which can affect how the model behaves and how it handles the features in the data.\n",
        "\n",
        "1. Ridge Regression (L2 Regularization):\n",
        "Penalty Term: The penalty in Ridge regression is based on the sum of the squares of the model coefficients.\n",
        "ùêΩ\n",
        "(\n",
        "ùúÉ\n",
        ")\n",
        "=\n",
        "Cost¬†Function\n",
        "+\n",
        "ùúÜ\n",
        "‚àë\n",
        "ùëó\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "ùúÉ\n",
        "ùëó\n",
        "2\n",
        "J(Œ∏)=Cost¬†Function+Œª\n",
        "j=1\n",
        "‚àë\n",
        "n\n",
        "‚Äã\n",
        " Œ∏\n",
        "j\n",
        "2\n",
        "‚Äã\n",
        "\n",
        "Where:\n",
        "ùúÜ\n",
        "Œª is the regularization parameter,\n",
        "ùúÉ\n",
        "ùëó\n",
        "Œ∏\n",
        "j\n",
        "‚Äã\n",
        "  are the coefficients of the features,\n",
        "The penalty term\n",
        "ùúÜ\n",
        "‚àë\n",
        "ùëó\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "ùúÉ\n",
        "ùëó\n",
        "2\n",
        "Œª‚àë\n",
        "j=1\n",
        "n\n",
        "‚Äã\n",
        " Œ∏\n",
        "j\n",
        "2\n",
        "‚Äã\n",
        "  is called the L2 penalty.\n",
        "Effect: Ridge regression shrinks the coefficients, making them smaller but does not eliminate them. It prevents large weights, but keeps all features in the model.\n",
        "When to Use: Ridge is useful when you have many features and suspect that most of them contribute to the prediction. It works well when the features are highly correlated or when there is a lot of noise in the data.\n",
        "Main Goal: Minimize the coefficients without forcing any of them to zero. The model retains all features, but their impact is limited.\n",
        "2. Lasso Regression (L1 Regularization):\n",
        "Penalty Term: The penalty in Lasso regression is based on the sum of the absolute values of the model coefficients.\n",
        "ùêΩ\n",
        "(\n",
        "ùúÉ\n",
        ")\n",
        "=\n",
        "Cost¬†Function\n",
        "+\n",
        "ùúÜ\n",
        "‚àë\n",
        "ùëó\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "‚à£\n",
        "ùúÉ\n",
        "ùëó\n",
        "‚à£\n",
        "J(Œ∏)=Cost¬†Function+Œª\n",
        "j=1\n",
        "‚àë\n",
        "n\n",
        "‚Äã\n",
        " ‚à£Œ∏\n",
        "j\n",
        "‚Äã\n",
        " ‚à£\n",
        "Where:\n",
        "ùúÜ\n",
        "Œª is the regularization parameter,\n",
        "ùúÉ\n",
        "ùëó\n",
        "Œ∏\n",
        "j\n",
        "‚Äã\n",
        "  are the model coefficients,\n",
        "The penalty term\n",
        "ùúÜ\n",
        "‚àë\n",
        "ùëó\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "‚à£\n",
        "ùúÉ\n",
        "ùëó\n",
        "‚à£\n",
        "Œª‚àë\n",
        "j=1\n",
        "n\n",
        "‚Äã\n",
        " ‚à£Œ∏\n",
        "j\n",
        "‚Äã\n",
        " ‚à£ is called the L1 penalty.\n",
        "Effect: Lasso regression encourages sparsity, meaning that it shrinks some coefficients to exactly zero, effectively removing the corresponding features from the model.\n",
        "When to Use: Lasso is useful when you suspect that only a subset of the features is important for the prediction. It is particularly useful for feature selection, as it automatically sets the coefficients of irrelevant features to zero.\n",
        "Main Goal: Perform feature selection by shrinking some feature coefficients to zero while keeping others.\n",
        "3. Elastic Net Regression:\n",
        "Penalty Term: Elastic Net is a combination of both L1 (Lasso) and L2 (Ridge) penalties.\n",
        "ùêΩ\n",
        "(\n",
        "ùúÉ\n",
        ")\n",
        "=\n",
        "Cost¬†Function\n",
        "+\n",
        "ùúÜ\n",
        "1\n",
        "‚àë\n",
        "ùëó\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "‚à£\n",
        "ùúÉ\n",
        "ùëó\n",
        "‚à£\n",
        "+\n",
        "ùúÜ\n",
        "2\n",
        "2\n",
        "‚àë\n",
        "ùëó\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "ùúÉ\n",
        "ùëó\n",
        "2\n",
        "J(Œ∏)=Cost¬†Function+Œª\n",
        "1\n",
        "‚Äã\n",
        "  \n",
        "j=1\n",
        "‚àë\n",
        "n\n",
        "‚Äã\n",
        " ‚à£Œ∏\n",
        "j\n",
        "‚Äã\n",
        " ‚à£+\n",
        "2\n",
        "Œª\n",
        "2\n",
        "‚Äã\n",
        "\n",
        "‚Äã\n",
        "  \n",
        "j=1\n",
        "‚àë\n",
        "n\n",
        "‚Äã\n",
        " Œ∏\n",
        "j\n",
        "2\n",
        "‚Äã\n",
        "\n",
        "Where:\n",
        "ùúÜ\n",
        "1\n",
        "Œª\n",
        "1\n",
        "‚Äã\n",
        "  controls the strength of the L1 penalty (Lasso),\n",
        "ùúÜ\n",
        "2\n",
        "Œª\n",
        "2\n",
        "‚Äã\n",
        "  controls the strength of the L2 penalty (Ridge),\n",
        "ùúÉ\n",
        "ùëó\n",
        "Œ∏\n",
        "j\n",
        "‚Äã\n",
        "  are the model coefficients.\n",
        "Effect: Elastic Net combines the strengths of both Lasso and Ridge:\n",
        "It allows for sparsity (like Lasso), but also shrinks coefficients (like Ridge).\n",
        "It can handle situations where there are highly correlated features, something that pure Lasso can struggle with.\n",
        "When to Use: Elastic Net is useful when you have many features, some of which may be correlated, and you want to combine the benefits of both Lasso and Ridge. It is particularly helpful when the number of features is greater than the number of observations or when features are highly correlated.\n",
        "Main Goal: Combine the advantages of feature selection (from Lasso) and regularization (from Ridge)."
      ],
      "metadata": {
        "id": "vGj4B9HrNoV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-7  When should we use Elastic Net instead of Lasso or Ridge ?"
      ],
      "metadata": {
        "id": "kL3goWB1OdIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We should use elastic net instead of lasso and ridge:\n",
        "1. when features are highly correlated\n",
        "2. when you have more features then samples\n",
        "3. when lasso alone does not perform well\n",
        "4. when you need a balance between lasso and ridge\n",
        "5. when you want a more stable model and reduced bias\n",
        "6. When You Want Feature Selection and Regularization Simultaneously\n",
        "7. when there is no clear preference between lasso and ridge\n"
      ],
      "metadata": {
        "id": "0kQ6MGQdOjqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-8 What is the impact of the regularization parameter (Œª) in Logistic Regression ?"
      ],
      "metadata": {
        "id": "-JjwYxbmPqC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The regularization parameter\n",
        "ùúÜ\n",
        "Œª (lambda) plays a crucial role in Logistic Regression with regularization (whether using L1 (Lasso), L2 (Ridge), or Elastic Net). It controls the strength of the regularization applied to the model, which directly impacts the model's performance, complexity, and generalization ability.\n"
      ],
      "metadata": {
        "id": "825HmsnwPxC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-9 What are the key assumptions of Logistic Regression ?"
      ],
      "metadata": {
        "id": "SvAqy1ltQHiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression, like any other statistical model, comes with a set of assumptions that are important for ensuring the model‚Äôs validity and reliability. While logistic regression is less restrictive compared to linear regression, it still relies on certain assumptions to produce meaningful results. Here are the key assumptions of logistic regression:\n",
        "\n",
        "1. Binary Outcome (Dependent Variable)\n",
        "2. Independence of observations\n",
        "3. Linearity of log ods\n",
        "4. no or little multicollinearity\n",
        "5. No high leverage outliers\n",
        "6. Large Sample Size\n",
        "7. No significant Measurment error for predictors\n",
        "8. the log-likelihood is maximum"
      ],
      "metadata": {
        "id": "WXAYxIGPQcMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-10 What are some alternatives to Logistic Regression for classification tasks ?"
      ],
      "metadata": {
        "id": "-uQIyUbLQ-yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For classification tasks, there are several alternatives to Logistic Regression. These models vary in complexity and performance depending on the nature of the data and the problem at hand. Some popular alternatives include:\n",
        "\n",
        "1. Decision Trees\n",
        "2. Random Forest\n",
        "3. Support vector machine\n",
        "4. k- Nearest neighbours\n",
        "5. naive bayes\n",
        "6. Neural Networks\n",
        "7. Gradient boosting machines\n",
        "8. ada boost\n",
        "9. Linear Discriminant Analysis (LDA)"
      ],
      "metadata": {
        "id": "3vxO5G_rRsek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-11 What are Classification Evaluation Metrics ?"
      ],
      "metadata": {
        "id": "hh3zabtqSN1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification evaluation metrics are used to assess the performance of classification models. These metrics help to understand how well a model is predicting the correct class labels. Depending on the nature of the task (binary or multi-class classification) and the dataset, different metrics are more appropriate for evaluating performance."
      ],
      "metadata": {
        "id": "0UQfJBQvQOMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-12 How does class imbalance affect Logistic Regression ?"
      ],
      "metadata": {
        "id": "9YYCvceCSfVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class imbalance can have a significant effect on the performance of Logistic Regression (and many other machine learning models). Here‚Äôs how it affects the model, as well as some potential solutions to address the problem:\n",
        "\n",
        "Impact of Class Imbalance on Logistic Regression\n",
        "1. Bias Toward the Majority Class\n",
        "2. poor evaluation metrics\n",
        "3. underestimated probability of minority class\n",
        "4. decision boundaries issue\n"
      ],
      "metadata": {
        "id": "vLFtX6OsSzCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-13 What is Hyperparameter Tuning in Logistic Regression ?"
      ],
      "metadata": {
        "id": "Xar-0qBRTOgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning in Logistic Regression refers to the process of selecting the optimal set of hyperparameters (parameters that are set before the model is trained) to improve the model‚Äôs performance. These hyperparameters are not learned from the data directly, but rather are manually set prior to training the model, and they can significantly affect the accuracy and generalizability of the model."
      ],
      "metadata": {
        "id": "UZKx5pOdTT_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-14 What are different solvers in Logistic Regression? Which one should be used ?"
      ],
      "metadata": {
        "id": "kA_7nWigThSm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Logistic Regression, the choice of solver refers to the algorithm used to optimize the cost function (log-likelihood) and find the model's coefficients (weights). Logistic Regression is typically optimized by iterative optimization algorithms, and the solver you choose can affect the speed of convergence, model accuracy, and scalability to large datasets.\n",
        "\n",
        "Here are the different solvers available in Logistic Regression, particularly in popular libraries like scikit-learn, and the scenarios in which each should be used:\n",
        "\n",
        "1. Newton-CG (Newton's Conjugate Gradient)\n",
        "2. LBFGS (Limited-memory Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno)\n",
        "3. Liblinear\n",
        "4. SAGA\n",
        "5. Sag (Stochastic Average Gradient)"
      ],
      "metadata": {
        "id": "KI--YpkbTqTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-15 How is Logistic Regression extended for multiclass classification ?"
      ],
      "metadata": {
        "id": "bwH4yBNjUIoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression, by default, is a binary classifier, meaning it predicts one of two possible outcomes (e.g., class 0 or class 1). However, many real-world problems require multiclass classification, where there are more than two possible classes.\n",
        "\n",
        "To handle multiclass classification in Logistic Regression, several strategies can be used to extend the binary logistic regression model. The two most common approaches are One-vs-Rest (OvR) and Multinomial Logistic Regression."
      ],
      "metadata": {
        "id": "BpJLtPpoUaqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-  What are the advantages and disadvantages of Logistic Regression?"
      ],
      "metadata": {
        "id": "je7Ad3c7Uk-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression is a popular and widely used model for binary and multiclass classification tasks. It has several advantages, but also some limitations. Below is a breakdown of the advantages and disadvantages of Logistic Regression:\n",
        "\n",
        "Advantages of Logistic Regression\n",
        "1. Simple to Implement and Interpret:\n",
        "\n",
        "2. Interpretability\n",
        "3. Efficiency\n",
        "\n",
        "4. Fast Training\n",
        "5. Probabilistic Output:\n",
        "6. Less Prone to Overfitting\n",
        "7. Multiclass Classification:\n",
        "\n",
        "8. Works Well for Linearly Separable Classes:\n",
        "\n",
        "\n",
        "Disadvantages of Logistic Regression\n",
        "1. Assumption of Linearity:\n",
        "\n",
        "2. Sensitive to Outliers:\n",
        "\n",
        "3. Cannot Model Complex Relationships:\n",
        "4. Limited Flexibility for Multiclass Problems:\n",
        "5. Poor Performance on Highly Imbalanced Data:\n",
        "6. Feature Engineering Required:\n",
        "7. Inability to Handle Large Feature Spaces Without Regularization:\n",
        "\n"
      ],
      "metadata": {
        "id": "u0VIu2rVVC_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-17 What are some use cases of Logistic Regression?"
      ],
      "metadata": {
        "id": "fLvbypvtWWDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression is a versatile statistical method used for various purposes, especially in binary classification problems. Here are some common use cases:\n",
        "\n",
        "1. Spam Detection (Email Classification)\n",
        "2. Medical diagonsis\n",
        "3. Credit Scoring and Risk Assessment\n",
        "4. Marketing Campaign Effectiveness\n",
        "5. customer churn protection\n",
        "6. Fraud Detection\n",
        "7. Political science and voting behaviour"
      ],
      "metadata": {
        "id": "v9-0ysoYWa0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-18 What is the difference between Softmax Regression and Logistic Regression ?"
      ],
      "metadata": {
        "id": "6f4FOeHzb9KL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both Softmax Regression and Logistic Regression are widely used for classification tasks, but they differ in the following ways:\n",
        "\n",
        "1. Type of Classification:\n",
        "Logistic Regression:\n",
        "It is used for binary classification, meaning the output can take only two values (e.g., \"yes\" or \"no,\" \"spam\" or \"not spam\"). The model predicts the probability of the positive class (usually denoted as 1) and uses the sigmoid function to map the output to a range between 0 and 1.\n",
        "Softmax Regression (Multinomial Logistic Regression):\n",
        "It is used for multiclass classification (more than two classes). Softmax regression generalizes logistic regression to handle multiple classes. Instead of predicting just one probability, it predicts a probability distribution across multiple classes, ensuring that the sum of all class probabilities is 1.\n",
        "2. Output:\n",
        "Logistic Regression:\n",
        "The output is a single probability (for the positive class). For binary classification, the model predicts the probability of the class being 1, and the probability of the other class (0) is simply 1 minus this probability.\n",
        "Softmax Regression:\n",
        "The output is a vector of probabilities for each class. For each class, the model predicts the probability that the input belongs to that class, and the sum of all these probabilities is 1.\n",
        "3. Mathematical Formulation:\n",
        "Logistic Regression:\n",
        "\n",
        "Logistic regression uses the sigmoid function for binary classification:\n",
        "ùëÉ\n",
        "(\n",
        "ùë¶\n",
        "=\n",
        "1\n",
        "‚à£\n",
        "ùëã\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "ùëí\n",
        "‚àí\n",
        "ùëß\n",
        "P(y=1‚à£X)=\n",
        "1+e\n",
        "‚àíz\n",
        "\n",
        "1\n",
        "‚Äã\n",
        "\n",
        "where\n",
        "ùëß\n",
        "=\n",
        "ùë§\n",
        "ùëá\n",
        "ùëã\n",
        "+\n",
        "ùëè\n",
        "z=w\n",
        "T\n",
        " X+b is the linear combination of input features\n",
        "ùëã\n",
        "X with the weights\n",
        "ùë§\n",
        "w and bias\n",
        "ùëè\n",
        "b.\n",
        "\n",
        "Softmax Regression:\n",
        "\n",
        "Softmax regression uses the softmax function for multiclass classification:\n",
        "ùëÉ\n",
        "(\n",
        "ùë¶\n",
        "=\n",
        "ùëò\n",
        "‚à£\n",
        "ùëã\n",
        ")\n",
        "=\n",
        "ùëí\n",
        "ùëß\n",
        "ùëò\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùêæ\n",
        "ùëí\n",
        "ùëß\n",
        "ùëñ\n",
        "P(y=k‚à£X)=\n",
        "‚àë\n",
        "i=1\n",
        "K\n",
        "‚Äã\n",
        " e\n",
        "z\n",
        "i\n",
        "‚Äã\n",
        "\n",
        "\n",
        "e\n",
        "z\n",
        "k\n",
        "‚Äã\n",
        "\n",
        "\n",
        "‚Äã\n",
        "\n",
        "where\n",
        "ùëß\n",
        "ùëò\n",
        "z\n",
        "k\n",
        "‚Äã\n",
        "  is the score for class\n",
        "ùëò\n",
        "k (a linear combination of inputs like in logistic regression) and\n",
        "ùêæ\n",
        "K is the number of classes. The softmax function ensures the output probabilities for each class sum to 1.\n",
        "\n",
        "4. Number of Parameters:\n",
        "Logistic Regression:\n",
        "Logistic regression has a single set of weights for the binary classification problem (one set of weights\n",
        "ùë§\n",
        "w and bias\n",
        "ùëè\n",
        "b).\n",
        "Softmax Regression:\n",
        "Softmax regression has a separate set of weights for each class (i.e., a weight vector\n",
        "ùë§\n",
        "ùëò\n",
        "w\n",
        "k\n",
        "‚Äã\n",
        "  for each class\n",
        "ùëò\n",
        "k in the case of\n",
        "ùêæ\n",
        "K classes). So, for\n",
        "ùêæ\n",
        "K classes, there will be\n",
        "ùêæ\n",
        "K sets of weights and biases.\n",
        "5. Loss Function:\n",
        "Logistic Regression:\n",
        "The loss function is binary cross-entropy (also known as log loss), which measures the difference between the predicted probability and the actual class label.\n",
        "Softmax Regression:\n",
        "The loss function is categorical cross-entropy (also known as multi-class log loss), which compares the predicted class probabilities across all classes to the actual class label.\n",
        "6. Applicability:\n",
        "Logistic Regression:\n",
        "Suitable for problems where you have only two classes (binary classification).\n",
        "Softmax Regression:\n",
        "Suitable for problems with more than two classes (multiclass classification). It generalizes logistic regression to handle multiple categories simultaneously.\n",
        "7. Example Use Cases:\n",
        "Logistic Regression:\n",
        "Spam detection, medical diagnosis (e.g., predicting if a patient has a disease or not).\n",
        "Softmax Regression:\n",
        "Image classification (e.g., classifying images into categories such as \"cat,\" \"dog,\" or \"bird\"), speech recognition, handwriting recognition, or any task where there are multiple categories to predict."
      ],
      "metadata": {
        "id": "nebchKu4cU61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-19  How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification ?"
      ],
      "metadata": {
        "id": "hv4cpWhkcpAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When choosing between One-vs-Rest (OvR) and Softmax for multiclass classification, the decision depends on several factors related to the dataset, model, and the problem at hand. Both techniques are widely used, but they approach multiclass classification in different ways. Let's look at both methods and consider when to use each.\n",
        "\n",
        "1. Softmax Regression\n",
        "How it works: Softmax is a single classifier that directly models the probability distribution over all classes. It outputs a probability for each class, and these probabilities sum to 1. The class with the highest probability is chosen as the predicted class.\n",
        "\n",
        "Pros:\n",
        "\n",
        "End-to-end approach: Softmax directly models multiclass classification as a single problem, optimizing the parameters for all classes together.\n",
        "Simplicity: Since it's a single classifier, there is less complexity in training and model management.\n",
        "Coherent probability distribution: The output is a valid probability distribution, meaning the sum of the probabilities for all classes is 1.\n",
        "Interpretability: The probabilities represent the model's confidence in each class, which can be valuable for decision-making.\n",
        "Cons:\n",
        "\n",
        "Class imbalance: Softmax may not perform well when the classes are highly imbalanced, as it might struggle to correctly classify underrepresented classes.\n",
        "Multi-class problems: It can become computationally expensive when you have a very large number of classes since the model has to output a probability for each class.\n",
        "When to use Softmax:\n",
        "\n",
        "When you have multiple classes (3 or more) and want to model them directly in a single classifier.\n",
        "When probabilities for each class are important and you want the sum of probabilities to equal 1.\n",
        "When the classes are not heavily imbalanced, or the model is robust enough to handle class imbalance.\n",
        "When your dataset is large enough to allow for the direct modeling of all classes together.\n",
        "2. One-vs-Rest (OvR)\n",
        "How it works: One-vs-Rest (OvR) is an approach where multiple binary classifiers are trained, one for each class. For each class, the classifier is trained to distinguish that class from all the others. During prediction, each classifier outputs a probability, and the class with the highest probability across all classifiers is chosen.\n",
        "\n",
        "Pros:\n",
        "\n",
        "Flexibility: OvR allows you to use binary classifiers (like logistic regression) for each class, and you can use any model that works well for binary classification.\n",
        "Better for imbalanced classes: OvR can handle class imbalance more easily because each classifier focuses on distinguishing one class from the others, which means the classifiers do not face the same class imbalance as a multi-class classifier.\n",
        "Easy to extend: It is easy to extend OvR to many classes by simply adding one classifier for each new class.\n",
        "Cons:\n",
        "\n",
        "Multiple classifiers: OvR requires training multiple binary classifiers, which can increase computational complexity and memory requirements, especially when there are many classes.\n",
        "No coherent probability distribution: Each classifier outputs a separate probability, so the probabilities do not necessarily sum to 1. This can make interpreting the results as a probability distribution more challenging.\n",
        "Potential for conflicting predictions: Since each classifier is independent, there is a possibility that more than one classifier could predict the same class as the most likely, leading to conflicts in predictions.\n",
        "When to use One-vs-Rest (OvR):\n",
        "\n",
        "When you need to use binary classifiers that work well for each individual class (e.g., logistic regression, SVM).\n",
        "When dealing with class imbalance, since OvR isolates each class from the others and can focus on the minority class more effectively.\n",
        "When the number of classes is large, and you prefer a more modular approach, where each binary classifier handles a specific class.\n",
        "When probability coherence is not critical (i.e., you're okay with the outputs being separate and not forming a proper probability distribution)."
      ],
      "metadata": {
        "id": "wV5te7FAc8JO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-20 How do we interpret coefficients in Logistic Regression?"
      ],
      "metadata": {
        "id": "leo8zhmwc-8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpreting the coefficients in Logistic Regression is essential to understand the relationship between the predictors (features) and the outcome (target variable). The coefficients represent how the predictors influence the log-odds of the target class, but interpreting them directly can be tricky since the outcome is not linear but instead relates to the logistic function."
      ],
      "metadata": {
        "id": "SwBIQmEsdR6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PRACTICAL QUESTIONS**"
      ],
      "metadata": {
        "id": "iQykdzvjWupx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUES-1 Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracy?"
      ],
      "metadata": {
        "id": "Q9R9igoQgwuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#load the dataset ,make a dataframe and devide into x and y\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "x = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "#train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "xtrain , xtest, ytrain , ytest = train_test_split(x ,y, test_size=0.3 , random_state= 42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(xtrain, ytrain)\n",
        "\n",
        "# Make predictions\n",
        "ypred = model.predict(xtest)\n",
        "\n",
        "# Calculate accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(ytest, ypred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PB2CCDeDgu0A",
        "outputId": "7638d2b2-8a87-438c-cf6c-3c5d569e75bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-2 Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracy?"
      ],
      "metadata": {
        "id": "gyzA6h0Ais7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# For simplicity, let's focus on binary classification (class 0 vs class 1)\n",
        "X = X[y != 2]  # Only keep classes 0 and 1\n",
        "y = y[y != 2]  # Only keep classes 0 and 1\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the data (important for regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create Logistic Regression model with L1 regularization (Lasso)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model's accuracy\n",
        "print(f'Model Accuracy with L1 Regularization (Lasso): {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print the coefficients of the model (for interpretation)\n",
        "print(\"Model Coefficients (L1 Regularization):\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17_Eyq6WgYW5",
        "outputId": "db293ff4-35a4-4838-c194-f545a8e021c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L1 Regularization (Lasso): 100.00%\n",
            "Model Coefficients (L1 Regularization):\n",
            "[[ 0.         -0.75916806  2.13830164  1.8255581 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-3 Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficients ?"
      ],
      "metadata": {
        "id": "wppVltFkgd6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# For simplicity, let's focus on binary classification (class 0 vs class 1)\n",
        "X = X[y != 2]  # Only keep classes 0 and 1\n",
        "y = y[y != 2]  # Only keep classes 0 and 1\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the data (important for regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create Logistic Regression model with L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model's accuracy\n",
        "print(f'Model Accuracy with L2 Regularization (Ridge): {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print the coefficients of the model (for interpretation)\n",
        "print(\"Model Coefficients (L2 Regularization):\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "id": "BFQPg7SahaoO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64a3bb6b-b5a1-493d-bd99-8f0ca2e1588a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L2 Regularization (Ridge): 100.00%\n",
            "Model Coefficients (L2 Regularization):\n",
            "[[ 0.74798703 -1.06767754  1.41584728  1.43119506]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-4 Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet') ?"
      ],
      "metadata": {
        "id": "ZR1guHrbgxyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# For simplicity, let's focus on binary classification (class 0 vs class 1)\n",
        "X = X[y != 2]  # Only keep classes 0 and 1\n",
        "y = y[y != 2]  # Only keep classes 0 and 1\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the data (important for regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create Logistic Regression model with Elastic Net regularization\n",
        "# ElasticNet uses a combination of L1 and L2 regularization (l1_ratio controls the mix).\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5)  # l1_ratio=0.5 is a 50-50 mix of L1 and L2\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model's accuracy\n",
        "print(f'Model Accuracy with Elastic Net Regularization: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print the coefficients of the model (for interpretation)\n",
        "print(\"Model Coefficients (Elastic Net Regularization):\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOUxApuMgvn9",
        "outputId": "c02f98f6-3d8b-45f9-87e8-4ddc66c6566c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with Elastic Net Regularization: 100.00%\n",
            "Model Coefficients (Elastic Net Regularization):\n",
            "[[ 0.53042926 -0.93594084  1.60456651  1.58486271]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-5 Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr' ?"
      ],
      "metadata": {
        "id": "W5lVXdhAhsoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the data (important for regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create Logistic Regression model with One-vs-Rest strategy (multi_class='ovr')\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model's accuracy\n",
        "print(f'Model Accuracy with One-vs-Rest Strategy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print the coefficients of the model (for interpretation)\n",
        "print(\"Model Coefficients (One-vs-Rest Strategy):\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPbk3_ILhr8N",
        "outputId": "e5b0e753-54ec-4df8-ed14-b966bf1ab99e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with One-vs-Rest Strategy: 91.11%\n",
            "Model Coefficients (One-vs-Rest Strategy):\n",
            "[[-0.68663736  1.31356156 -1.53218577 -1.33167409]\n",
            " [ 0.12843288 -1.36457652  0.51126967 -0.68227209]\n",
            " [ 0.10252227 -0.05605024  1.72030107  2.31388888]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-  Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracy ?"
      ],
      "metadata": {
        "id": "dhvv_pshh72b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the data (important for regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Set up the hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2']  # Type of regularization\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, verbose=1, n_jobs=-1)\n",
        "\n",
        "# Train the model with GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and the best accuracy score\n",
        "best_params = grid_search.best_params_\n",
        "best_accuracy = grid_search.best_score_\n",
        "\n",
        "# Print the best parameters and accuracy\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Best Cross-Validation Accuracy: {best_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Test the model with the best parameters on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Set Accuracy: {test_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQpX6Cz5h6TD",
        "outputId": "78ffcacd-1f63-486d-a58c-4af659e2b92a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "Best Hyperparameters: {'C': 100, 'penalty': 'l2'}\n",
            "Best Cross-Validation Accuracy: 95.24%\n",
            "Test Set Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "30 fits failed out of a total of 60.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan 0.64761905        nan 0.84761905        nan 0.9047619\n",
            "        nan 0.94285714        nan 0.94285714        nan 0.95238095]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-7 Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracy ?"
      ],
      "metadata": {
        "id": "k5vV7wU3jPBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the data (important for regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2']  # Regularization type (L1 or L2)\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV to find the best hyperparameters\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Train the model using GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "# Make predictions with the best model\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy of the best model\n",
        "print(f\"Accuracy of the Best Model: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkJ_at92jM57",
        "outputId": "a5bb3fa8-7115-4c1e-87af-b8664138329f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 100, 'penalty': 'l1'}\n",
            "Accuracy of the Best Model: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-8 Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "accuracy ?\n"
      ],
      "metadata": {
        "id": "3t__NJTXjiUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Let's use the Iris dataset as an example\n",
        "# Load the dataset\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Print the first few rows of the dataset\n",
        "print(\"Dataset (first 5 rows):\")\n",
        "print(df.head())\n",
        "\n",
        "# Assume the last column is the target (y) and the rest are features (X)\n",
        "X = df.iloc[:, :-1].values  # All columns except the last one (features)\n",
        "y = df.iloc[:, -1].values   # The last column (target)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the data (important for regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create and train a Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear', multi_class='ovr')  # 'liblinear' solver works well for small datasets\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy of the model\n",
        "print(f'Model Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Optionally, you can print the coefficients of the model\n",
        "print(\"Model Coefficients:\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7Z3K2CjjyCE",
        "outputId": "c0543612-1e2b-43c1-ed2f-9e6d4b219c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset (first 5 rows):\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  target\n",
            "0                5.1               3.5                1.4               0.2       0\n",
            "1                4.9               3.0                1.4               0.2       0\n",
            "2                4.7               3.2                1.3               0.2       0\n",
            "3                4.6               3.1                1.5               0.2       0\n",
            "4                5.0               3.6                1.4               0.2       0\n",
            "Model Accuracy: 91.11%\n",
            "Model Coefficients:\n",
            "[[-0.68663736  1.31356156 -1.53218577 -1.33167409]\n",
            " [ 0.12843288 -1.36457652  0.51126967 -0.68227209]\n",
            " [ 0.10252227 -0.05605024  1.72030107  2.31388888]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-9 Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracy ?"
      ],
      "metadata": {
        "id": "TZ4iIwNjkKFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Print the first few rows of the dataset\n",
        "print(\"Dataset (first 5 rows):\")\n",
        "print(df.head())\n",
        "\n",
        "# Assume the last column is the target (y) and the rest are features (X)\n",
        "X = df.iloc[:, :-1].values  # All columns except the last one (features)\n",
        "y = df.iloc[:, -1].values   # The last column (target)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the data (important for regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "model = LogisticRegression(multi_class='ovr', max_iter=100)\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "param_dist = {\n",
        "    'C': uniform(0.001, 10),  # Regularization strength\n",
        "    'penalty': ['l1', 'l2'],  # Regularization type\n",
        "    'solver': ['liblinear', 'saga']  # Solvers that support L1 regularization\n",
        "}\n",
        "\n",
        "# Apply RandomizedSearchCV to find the best hyperparameters\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist,\n",
        "                                   n_iter=100, cv=5, scoring='accuracy', random_state=42, n_jobs=-1)\n",
        "\n",
        "# Train the model using RandomizedSearchCV\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters from the RandomizedSearchCV\n",
        "best_params = random_search.best_params_\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "# Make predictions with the best model\n",
        "y_pred = random_search.best_estimator_.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy of the best model\n",
        "print(f\"Accuracy of the Best Model: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfOpN5GFkGwU",
        "outputId": "5c246470-5649-4235-d74b-41d8bfdc0648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset (first 5 rows):\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  target\n",
            "0                5.1               3.5                1.4               0.2       0\n",
            "1                4.9               3.0                1.4               0.2       0\n",
            "2                4.7               3.2                1.3               0.2       0\n",
            "3                4.6               3.1                1.5               0.2       0\n",
            "4                5.0               3.6                1.4               0.2       0\n",
            "Best Hyperparameters: {'C': 7.320939418114051, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Accuracy of the Best Model: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-10 Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy ?"
      ],
      "metadata": {
        "id": "KmCGCH_Xkfgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Print the first few rows of the dataset\n",
        "print(\"Dataset (first 5 rows):\")\n",
        "print(df.head())\n",
        "\n",
        "# Assume the last column is the target (y) and the rest are features (X)\n",
        "X = df.iloc[:, :-1].values  # All columns except the last one (features)\n",
        "y = df.iloc[:, -1].values   # The last column (target)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the data (important for regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the base Logistic Regression model\n",
        "log_reg = LogisticRegression(solver='liblinear', max_iter=100)\n",
        "\n",
        "# Create an One-vs-One classifier\n",
        "ovo_classifier = OneVsOneClassifier(log_reg)\n",
        "\n",
        "# Train the One-vs-One classifier\n",
        "ovo_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ovo_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy of the model\n",
        "print(f\"Accuracy of One-vs-One (OvO) Multiclass Logistic Regression: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdxvG5CLkaGg",
        "outputId": "2ace3074-f64c-4eb5-c0f3-e3d97562c9ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset (first 5 rows):\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  target\n",
            "0                5.1               3.5                1.4               0.2       0\n",
            "1                4.9               3.0                1.4               0.2       0\n",
            "2                4.7               3.2                1.3               0.2       0\n",
            "3                4.6               3.1                1.5               0.2       0\n",
            "4                5.0               3.6                1.4               0.2       0\n",
            "Accuracy of One-vs-One (OvO) Multiclass Logistic Regression: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-11 Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classification ?"
      ],
      "metadata": {
        "id": "pUcDQZHBkzJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using seaborn heatmap\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "aU8yVkDFkx_o",
        "outputId": "730dffec-475b-4f05-ece6-a928618d6c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 84.67%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-12 Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-Score ?"
      ],
      "metadata": {
        "id": "Gvv0J_8FlHc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the performance metrics\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9rR-kArk_lQ",
        "outputId": "22c29462-194d-4154-cd27-02cb07f0b1b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 84.67%\n",
            "Precision: 0.89\n",
            "Recall: 0.82\n",
            "F1-Score: 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-13 Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performance ?"
      ],
      "metadata": {
        "id": "dtP0T-owlXHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Generate an imbalanced synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2,\n",
        "                           weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Check the distribution of the target variable (imbalanced)\n",
        "print(f\"Class distribution in the dataset: {np.bincount(y)}\")\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model with class weights\n",
        "model = LogisticRegression(class_weight='balanced', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1-score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the performance metrics\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQyvFG8VlTj2",
        "outputId": "9aec6a6e-44d8-4af3-f39f-5819a0d37a10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution in the dataset: [897 103]\n",
            "Model Accuracy: 87.33%\n",
            "Precision: 0.50\n",
            "Recall: 0.84\n",
            "F1-Score: 0.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-14 Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performance ?"
      ],
      "metadata": {
        "id": "uz7MZDwIl1Tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Titanic dataset\n",
        "url = \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Print column names to check for issues\n",
        "print(\"Columns in the dataset:\", df.columns)\n",
        "\n",
        "# Handle missing values\n",
        "df['Age'] = df['Age'].fillna(df['Age'].mean())  # Fill missing 'Age' with mean\n",
        "\n",
        "# Encode categorical data (we will encode 'Sex')\n",
        "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})  # Male: 0, Female: 1\n",
        "\n",
        "# Select features (X) and target (y)\n",
        "X = df[['Pclass', 'Sex', 'Age', 'Fare']]  # Features\n",
        "y = df['Survived']  # Target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the Logistic Regression model: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMd91VjVq_pD",
        "outputId": "678575ab-d190-4441-ab16-8b70ff13c202"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in the dataset: Index(['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'Siblings/Spouses Aboard',\n",
            "       'Parents/Children Aboard', 'Fare'],\n",
            "      dtype='object')\n",
            "Accuracy of the Logistic Regression model: 0.7584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-15 Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scaling ?"
      ],
      "metadata": {
        "id": "MsCJ19RmrTVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Titanic dataset\n",
        "url = \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "df['Age'] = df['Age'].fillna(df['Age'].mean())  # Fill missing 'Age' with mean\n",
        "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})  # Encode 'Sex' column: Male = 0, Female = 1\n",
        "\n",
        "# Select features (X) and target (y)\n",
        "X = df[['Pclass', 'Sex', 'Age', 'Fare']]  # Features\n",
        "y = df['Survived']  # Target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ----------- Logistic Regression without Feature Scaling -----------\n",
        "\n",
        "# Train Logistic Regression without scaling\n",
        "model_no_scaling = LogisticRegression(random_state=42)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model's performance without scaling\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "print(f\"Accuracy without feature scaling: {accuracy_no_scaling:.4f}\")\n",
        "\n",
        "# ----------- Logistic Regression with Feature Scaling (Standardization) -----------\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression with scaling\n",
        "model_with_scaling = LogisticRegression(random_state=42)\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model's performance with scaling\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "print(f\"Accuracy with feature scaling: {accuracy_with_scaling:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hxbw1pWHrRZ8",
        "outputId": "2d99e567-24d8-42f1-f055-986ae760961d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without feature scaling: 0.7640\n",
            "Accuracy with feature scaling: 0.7584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques- Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score ?"
      ],
      "metadata": {
        "id": "G7hat2BCrmJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Titanic dataset\n",
        "url = \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "df['Age'] = df['Age'].fillna(df['Age'].mean())  # Fill missing 'Age' with mean\n",
        "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})  # Encode 'Sex' column: Male = 0, Female = 1\n",
        "\n",
        "# Select features (X) and target (y)\n",
        "X = df[['Pclass', 'Sex', 'Age', 'Fare']]  # Features\n",
        "y = df['Survived']  # Target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict probabilities (needed for ROC-AUC score)\n",
        "y_prob = model.predict_proba(X_test_scaled)[:, 1]  # Get the probability for the positive class\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(f\"ROC-AUC score: {roc_auc:.4f}\")\n",
        "\n",
        "# Plot ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line for random classifier\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "9nu1kN_SrkJ4",
        "outputId": "c26ce70c-c0e1-466f-c776-733a48f3ad53"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC score: 0.8133\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-17 Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "accuracy ?"
      ],
      "metadata": {
        "id": "KXFkjkrrr8MK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Titanic dataset\n",
        "url = \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "df['Age'] = df['Age'].fillna(df['Age'].mean())  # Fill missing 'Age' with mean\n",
        "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})  # Encode 'Sex' column: Male = 0, Female = 1\n",
        "\n",
        "# Select features (X) and target (y)\n",
        "X = df[['Pclass', 'Sex', 'Age', 'Fare']]  # Features\n",
        "y = df['Survived']  # Target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression with custom learning rate (C=0.5)\n",
        "model = LogisticRegression(C=0.5, random_state=42)  # C is the inverse of the regularization strength\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model's performance\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy of Logistic Regression model with C=0.5: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jpssuM3r5N3",
        "outputId": "48a5bfea-5912-4f64-eeae-e16fdf7b6065"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Logistic Regression model with C=0.5: 0.7584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-18 Write a Python program to train Logistic Regression and identify important features based on model\n",
        "coefficients ?"
      ],
      "metadata": {
        "id": "-1fS6IpFsbCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Load Titanic dataset\n",
        "url = \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "df['Age'] = df['Age'].fillna(df['Age'].mean())  # Fill missing 'Age' with mean\n",
        "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})  # Encode 'Sex' column: Male = 0, Female = 1\n",
        "\n",
        "# Select features (X) and target (y)\n",
        "X = df[['Pclass', 'Sex', 'Age', 'Fare']]  # Features\n",
        "y = df['Survived']  # Target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get model coefficients and the corresponding feature names\n",
        "coefficients = model.coef_[0]  # Get the coefficients for the features\n",
        "feature_names = X.columns  # Get the feature names\n",
        "\n",
        "# Combine the feature names with their coefficients\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': coefficients,\n",
        "    'Importance': np.abs(coefficients)  # The higher the absolute coefficient, the more important the feature\n",
        "})\n",
        "\n",
        "# Sort the features by importance\n",
        "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print the feature importance\n",
        "print(\"Feature Importance based on Logistic Regression Coefficients:\")\n",
        "print(feature_importance)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxAgEJVwsVg6",
        "outputId": "c73addae-9e5e-4a1e-cd4a-f7b01c146eb4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importance based on Logistic Regression Coefficients:\n",
            "  Feature  Coefficient  Importance\n",
            "1     Sex     1.282458    1.282458\n",
            "0  Pclass    -0.960348    0.960348\n",
            "2     Age    -0.395350    0.395350\n",
            "3    Fare     0.055853    0.055853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-19 Write a Python program to train Logistic Regression and evaluate its performance using Cohen‚Äôs Kappa\n",
        "Score?"
      ],
      "metadata": {
        "id": "Ds2l5q7ItUZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
        "\n",
        "# Load Titanic dataset\n",
        "url = \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "df['Age'] = df['Age'].fillna(df['Age'].mean())  # Fill missing 'Age' with mean\n",
        "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})  # Encode 'Sex' column: Male = 0, Female = 1\n",
        "\n",
        "# Select features (X) and target (y)\n",
        "X = df[['Pclass', 'Sex', 'Age', 'Fare']]  # Features\n",
        "y = df['Survived']  # Target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model's performance using Cohen's Kappa score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy of Logistic Regression model: {accuracy:.4f}\")\n",
        "print(f\"Cohen's Kappa score: {kappa_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZT_ZYcis4uA",
        "outputId": "64f0620f-b372-4949-c822-37b727bbe967"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Logistic Regression model: 0.7584\n",
            "Cohen's Kappa score: 0.4648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-20 Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "classification ?"
      ],
      "metadata": {
        "id": "dOcjKVU6tkU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Titanic dataset\n",
        "url = \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "df['Age'] = df['Age'].fillna(df['Age'].mean())  # Fill missing 'Age' with mean\n",
        "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})  # Encode 'Sex' column: Male = 0, Female = 1\n",
        "\n",
        "# Select features (X) and target (y)\n",
        "X = df[['Pclass', 'Sex', 'Age', 'Fare']]  # Features\n",
        "y = df['Survived']  # Target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get predicted probabilities for the positive class\n",
        "y_prob = model.predict_proba(X_test_scaled)[:, 1]  # Probabilities for class 1 (survived)\n",
        "\n",
        "# Compute Precision-Recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='blue', label='Precision-Recall curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for Logistic Regression')\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "gUCjy8ebtijE",
        "outputId": "f712df4a-8002-4458-8f15-5fd16bf4583b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-21 Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "their accuracy ?"
      ],
      "metadata": {
        "id": "uWrP7Dwht2jD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Titanic dataset\n",
        "url = \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "df['Age'] = df['Age'].fillna(df['Age'].mean())  # Fill missing 'Age' with mean\n",
        "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})  # Encode 'Sex' column: Male = 0, Female = 1\n",
        "\n",
        "# Select features (X) and target (y)\n",
        "X = df[['Pclass', 'Sex', 'Age', 'Fare']]  # Features\n",
        "y = df['Survived']  # Target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# List of solvers to test\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "# Store results\n",
        "results = {}\n",
        "\n",
        "# Train Logistic Regression with different solvers and evaluate accuracy\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, random_state=42)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "    # Evaluate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results[solver] = accuracy\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(\"Accuracy Comparison for different solvers:\")\n",
        "for solver, accuracy in results.items():\n",
        "    print(f\"Solver: {solver}, Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4MlgjRjtyK0",
        "outputId": "f3bfa601-f432-4992-d4a2-d1fbf6ee8ba7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Comparison for different solvers:\n",
            "Solver: liblinear, Accuracy: 0.7584\n",
            "Solver: saga, Accuracy: 0.7584\n",
            "Solver: lbfgs, Accuracy: 0.7584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-22 Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "Correlation Coefficient (MCC) ?"
      ],
      "metadata": {
        "id": "Yz9bk_XLuLCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Load Titanic dataset\n",
        "url = \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "df['Age'] = df['Age'].fillna(df['Age'].mean())  # Fill missing 'Age' with mean\n",
        "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})  # Encode 'Sex' column: Male = 0, Female = 1\n",
        "\n",
        "# Select features (X) and target (y)\n",
        "X = df[['Pclass', 'Sex', 'Age', 'Fare']]  # Features\n",
        "y = df['Survived']  # Target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model using Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_CCemMVuKUA",
        "outputId": "87598b74-4523-4339-810f-42d820b6cd47"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.4710\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-23 Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scaling ?"
      ],
      "metadata": {
        "id": "1WUweiUdubKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Titanic dataset\n",
        "url = \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "df['Age'] = df['Age'].fillna(df['Age'].mean())  # Fill missing 'Age' with mean\n",
        "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})  # Encode 'Sex' column: Male = 0, Female = 1\n",
        "\n",
        "# Select features (X) and target (y)\n",
        "X = df[['Pclass', 'Sex', 'Age', 'Fare']]  # Features\n",
        "y = df['Survived']  # Target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression on raw data\n",
        "model_raw = LogisticRegression(random_state=42)\n",
        "model_raw.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the raw data\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy on raw data\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression on standardized data\n",
        "model_scaled = LogisticRegression(random_state=42)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the standardized data\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate accuracy on standardized data\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(f\"Accuracy on Raw Data: {accuracy_raw:.4f}\")\n",
        "print(f\"Accuracy on Standardized Data: {accuracy_scaled:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhNf6A6cuaA8",
        "outputId": "2401336f-0e3f-41e1-a0c0-fd1514e6279e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Raw Data: 0.7640\n",
            "Accuracy on Standardized Data: 0.7584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-24 Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "cross-validation ?"
      ],
      "metadata": {
        "id": "wDdaviNnurX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Titanic dataset\n",
        "url = \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "df['Age'] = df['Age'].fillna(df['Age'].mean())  # Fill missing 'Age' with mean\n",
        "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})  # Encode 'Sex' column: Male = 0, Female = 1\n",
        "\n",
        "# Select features (X) and target (y)\n",
        "X = df[['Pclass', 'Sex', 'Age', 'Fare']]  # Features\n",
        "y = df['Survived']  # Target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "log_reg = LogisticRegression(solver='liblinear', random_state=42)\n",
        "\n",
        "# Set up the parameter grid for C\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# Perform GridSearchCV with 5-fold cross-validation to find the optimal C\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Print the best value of C and the corresponding accuracy\n",
        "print(f\"Optimal C: {grid_search.best_params_['C']}\")\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Use the best model to make predictions on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate accuracy on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy on test set: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8IVVyn3upfx",
        "outputId": "160954c1-24e9-4e5c-c768-b739ceb33485"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal C: 10\n",
            "Best cross-validation accuracy: 0.8054\n",
            "Accuracy on test set: 0.7697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-25 Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "make predictions ?"
      ],
      "metadata": {
        "id": "hlrmOXxDu8wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib  # For saving and loading models\n",
        "\n",
        "# Load Titanic dataset\n",
        "url = \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "df['Age'] = df['Age'].fillna(df['Age'].mean())  # Fill missing 'Age' with mean\n",
        "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})  # Encode 'Sex' column: Male = 0, Female = 1\n",
        "\n",
        "# Select features (X) and target (y)\n",
        "X = df[['Pclass', 'Sex', 'Age', 'Fare']]  # Features\n",
        "y = df['Survived']  # Target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Save the trained model using joblib\n",
        "joblib.dump(model, 'logistic_regression_model.joblib')\n",
        "\n",
        "# Save the scaler as well for future predictions (scaling the data in the same way)\n",
        "joblib.dump(scaler, 'scaler.joblib')\n",
        "\n",
        "# Load the trained model from the disk\n",
        "loaded_model = joblib.load('logistic_regression_model.joblib')\n",
        "loaded_scaler = joblib.load('scaler.joblib')\n",
        "\n",
        "# Use the loaded model to make predictions\n",
        "X_test_scaled_loaded = loaded_scaler.transform(X_test)  # Ensure to scale the test data in the same way\n",
        "y_pred = loaded_model.predict(X_test_scaled_loaded)\n",
        "\n",
        "# Evaluate accuracy of the loaded model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy of the loaded model: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8Xtd6Eou6ux",
        "outputId": "ae468f70-99c7-454b-b894-50c88291400e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the loaded model: 0.7584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qqFC_iZdvNhH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}